What does the analytic case look like?  Say we have

a x + (b x')' = 0

a x + b' x' + b x'' = 0


x = f y
x' = f' y + f y'

a f y + (b f' y + b f y')' = 0
a f y + (b f' y)' + (b f y')' = 0
a f y + (b f' y)' + (b f y')' = 0

-------------------------------------

x(t) = C0 + int_{u=0 to t} (C1 + (int_{v=0 to u} -a(v) dv))/b(u) du

aa' = a
c = aa/b
cc' = c
x = cc

a x + (b x')' = 0
a cc + (b cc')' = 0
a cc + (b c)' = 0
a cc + aa' = 0
a cc + a = 0

-----------------------------------------

Li, Ri

xi, xj, xk

xi = Li a + Ri b
xk = Lk a + Rk b
xj = Lj a + Rj b

A = ( Li Ri
      Lk Rk )

Ainv = 1/(Li Rk - Lk Ri) (  Rk -Ri )
                         ( -Lk  Li )

LR[j]' Ainv

dot(LR[j],linalg.solve(vstack([LR[i],LR[k]]),(xi,xk)))

-------------------------------------------------

We can define an abstract notion of energy link between two variables i < k as E(i,j),
where we can linearly link i < j < k if we know E(i,j), E(j,k).  Initially, of course,
we know E(i,i+1) for all i.  E(i,j)E(j,k) can be reduced to E(i,k) in O(1) time.

Now consider prefix path up to i0 = j0, followed by lower path i1,i2,... and lower path j1,j2...
We know E(i(a),i(a+1)) and the same for j.  That's enough to walk down paths, but not enough
to walk up paths.  When the latter happens, we have lower path j0,j1, upper path j0=i0,i1,...,in=j1,
and we need to know E(i1,j1).  Which is interesting, because that's also E(i1,in).

The problem is that storing E(ia,in) doesn't work, because we keep adding onto the end.  This would
be fine if we only needed up update E(i1,in), but we need to deal with the others as well.

The situation reminds me of triangulating convex polygons, but unfortunately the space of those
triangulations is nasty to walk around in.

-------------------------------------------------------

Let's talk about Cholesky factorizations.  They look like

  A = L L'

where L is computable in one forward pass.  What do they equations look like, again?  We have

  A(i,j) = sum_k L(i,k) L(j,k)
  A(i,i) = L(i,i)^2 + L(i,i-1)^2
  A(i,i+1) = L(i,i) L(i+1,i)

or, reversed

  L(i,i) = sqrt(A(i,i) - L(i,i-1)^2)
  L(i+1,i) = A(i,i+1)/L(i,i)

  L(i+1,i+1) = sqrt(A(i+1,i+1) - L(i+1,i)^2) = sqrt(A(i+1,i+1) - A(i,i+1)^2/L(i,i)^2)
  L(i+1,i+1)^2 = A(i+1,i+1) - A(i,i+1)^2/L(i,i)^2

Say we set B = D A D for D diagonal.  We have

  B(i,i) = D(i)^2 A(i,i)
  B(i,i+1) = sum_kl D(i,k) A(k,l) D(l,i+1) = D(i) D(i+1) A(i,i+1)

-------------
Say the ratio between A(i,i) and A(i,i+-1) is bounded.  Let's look for a factorization of the form
SLL'S = B, where S is diagonal:

  S(i+1)^2 L(i+1,i+1)^2 = D(i+1)^2 A(i+1,i+1) - D(i)^2 D(i+1)^2 A(i,i+1)^2 / (S(i)^2 L(i,i)^2)
  L(i+1,i+1)^2 = D(i+1)^2/S(i+1)^2 A(i+1,i+1) - D(i)^2 D(i+1)^2 A(i,i+1)^2 / (S(i+1)^2 S(i)^2 L(i,i)^2)

Set

  S(i+1)^2 = D(i+1)^2 A(i+1,i+1) / 2
  D(i+1)^2 = S(i+1)^2 S(i)^2 / (D(i)^2 A(i,i+1)^2)

to get

  L(i+1,i+1)^2 = 2 - 1/L(i,i)^2

Assume A(i,i+1) != 0 and that overflow never occurs (which we'll have to arrange specially).  Then our
formulas for S(i) and D(i) are stable since they involve multiplications only.  Moreover, we now have
an analytic formula for L(i,i) in terms of L(0,0):

  j = i+1
  L(i,i) = (1 + (L(0,0)-1) j) / (2 - j - (L(0,0)-1) j)
         = (1 + (L(0,0)-1) (i+1)) / (2 - i - 1 - (L(0,0)-1) (i+1))
         = (L(0,0)i - i + L(0,0)) / (2 - L(0,0)i - L(0,0))

This formula should hold even if we're only computing part of the Cholesky decomposition, 

------------

Is there a simpler way to view this scaling process?  Can we ignore the Cholesky part entirely and simply
apply it to A.  Clearly no, which means something contradictory is going on and I made a mistake.  Ah, I see it.  Damn.

LL(i+1) = 1 - O(i)/LL(i)
LL(i+2) = 1 - O(i+1)/(1 - O(i)/LL(i))

Uh oh, this is basically a general continued fraction.  I should probably abandon hope for this direction.

-----------------------------------------------------------------------------------------------------------

Define the continuant recurrence by

  f(-2) = 0
  f(-1) = 1
  f(0) = A(0,0)
  f(i) = A(i,i) f(i-1) - A(i,i-1)^2 f(i-2)

Assume

  A(i,i) = 1
  b(i) = A(i,i-1)

  f(i) = f(i-1) - b(i) f(i-2)
  f(0) = 1
  f(1) = 1 - b(1)
  f(2) = 1 - b(1) - b(2)
  f(3) = 1 - b(1) - b(2) - b(3) (1 - b(1))
       = 1 - b(1) - b(2) - b(3) + b(1)b(3)

----------------------------------------------------------------------------------------------------------

Here's an idea.  Let's say we had

  B = A^{-1}

in the sense that we know B(i,j) in O(1) time for any i,j.  Can we solve our problem?
Given i < j < k, consider

  y_i = B e_i
  y_i(i) = B(i,i) > 0 # by positive semidefiniteness

Thus, we have

  (B e_i/B(i,i))_i = 1

Now construct y s.t.

  y(l) = 0 unless l = i or k
  (B y)(i) = x(i) = B(i,i) y(i) + B(i,k) y(k)
  (B y)(k) = x(k) = B(k,i) y(i) + B(k,k) y(k)

This 2x2 system is a minor of B, and therefore stable, so we'd be done.

------------------------------------------------------------------------------------------------

Following "On an inverse formula of a tridiagonal matrix", define

  a(i) = A(i,i)
  b(i) = A(i,i+1)
  f(-1) = 1
  f(i) = -b(i)/(f(i-1)b(i-1) + a(i))
  p(i,k) = prod_{j=i to k-1} f(j) for i <= k
         = f(i) f(i+1) ... f(k-1) for i <= k
  v(i,i) = b(i-1) f(i-1) + a(i) + b(i) 

--------------------------------------------------------------------------------

q[i] = bb[i]^2 dd[i] ee[i]
dd[i] = d[i+1]...d[n]
ee[i] = e[i]...e[n]
bb[i] = bb[0]...bb[i-1]

d[i] = a[i]-b[i]^2/d[i+1]
e[i] = a[i]-b[i-1]^2/e[i-1]

q[i]-q[i+1] = bb[i]^2 dd[i] ee[i] - bb[i+1]^2 dd[i+1] ee[i+1] = bb[i]^2 dd[i+1] ee[i+1] (d[i+1] e[i] - b[i]^2)
